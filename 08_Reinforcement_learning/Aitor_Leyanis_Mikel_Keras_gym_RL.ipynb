{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proyecto práctico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consideraciones a tener en cuenta:\n",
    "\n",
    "- El entorno sobre el que trabajaremos será _Space Invaders_ y el algoritmo que usaremos será _DQN_.\n",
    "\n",
    "- Para nuestro ejercicio, una solución óptima será alcanzada cuando el agente consiga una **media de recompensa por encima de 20 puntos en modo test**. Por ello, esta media de la recompensa se calculará a partir del código de test en la última celda del notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este proyecto práctico consta de tres partes:\n",
    "\n",
    "   1) Implementar la red neuronal que se usará en la solución\n",
    "    \n",
    "   2) Implementar las distintas piezas de la solución DQN\n",
    "    \n",
    "   3) Justificar la respuesta en relación a los resultados obtenidos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANTE:\n",
    "\n",
    "- Si no se consigue una puntuación óptima, responder sobre la mejor puntuación obtenida.\n",
    "\n",
    "- Para entrenamientos largos, recordad que podéis usar checkpoints de vuestros modelos para retomar los entrenamientos. En este caso, recordad cambiar los parámetros adecuadamente (sobre todo los relacionados con el proceso de exploración).\n",
    "\n",
    "- Necesitaréis instalar `gymnasium[atari,accept-rom-license]`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# import numpy as np\n",
    "# #import gymnasium as gym\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuración base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT_SHAPE = (84, 84)\n",
    "# WINDOW_LENGTH = 4\n",
    "\n",
    "# env_name = 'SpaceInvaders-v4'\n",
    "# env = gym.make(env_name)\n",
    "\n",
    "# np.random.seed(123)\n",
    "# nb_actions = env.action_space.n\n",
    "# state, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Implementación de la red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if IN_COLAB:\n",
    "#   %pip install gym==0.17.3\n",
    "#   %pip install git+https://github.com/Kojoley/atari-py.git\n",
    "#   %pip install keras-rl2==1.0.5\n",
    "#   %pip install tensorflow==2.8\n",
    "# else:\n",
    "#   %pip install gym==0.17.3\n",
    "#   %pip install git+https://github.com/Kojoley/atari-py.git\n",
    "#   %pip install pyglet==1.5.0\n",
    "#   %pip install h5py==3.1.0\n",
    "#   %pip install Pillow==9.5.0\n",
    "#   %pip install keras-rl2==1.0.5\n",
    "#   %pip install Keras==2.2.4\n",
    "#   %pip install tensorflow==2.5.3\n",
    "#   %pip install torch==2.0.1\n",
    "#   %pip install agents==1.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor Flow Version: 2.5.3\n",
      "WARNING:tensorflow:From C:\\Users\\plane\\AppData\\Local\\Temp\\ipykernel_21232\\1319947564.py:9: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "\n",
    "import tensorflow.keras\n",
    "import tensorflow as tf\n",
    "\n",
    "print(f\"Tensor Flow Version: {tf.__version__}\")\n",
    "print(\"GPU is\", \"available\" if tf.test.is_gpu_available() else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import gym\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Convolution2D, Permute, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.core import Processor\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE = (84, 84)\n",
    "WINDOW_LENGTH = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this example, we need to preprocess the observations\n",
    "class AtariProcessor(Processor):\n",
    "    def process_observation(self, observation):\n",
    "        assert observation.ndim == 3  # (height, width, channel)\n",
    "        img = Image.fromarray(observation)\n",
    "        img = img.resize(INPUT_SHAPE).convert('L')\n",
    "        processed_observation = np.array(img)\n",
    "        assert processed_observation.shape == INPUT_SHAPE\n",
    "        return processed_observation.astype('uint8')\n",
    "\n",
    "    def process_state_batch(self, batch):\n",
    "        processed_batch = batch.astype('float32') / 255.\n",
    "        return processed_batch\n",
    "\n",
    "    def process_reward(self, reward):\n",
    "        return np.clip(reward, -1., 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "env_name = 'SpaceInvaders-v4'\n",
    "env = gym.make(env_name)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de acciones disponibles: 6\n",
      "\n",
      "\n",
      "Formato de las observaciones:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Box(0, 255, (210, 160, 3), uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Numero de acciones disponibles: \" + str(nb_actions))\n",
    "print(\"\\n\")\n",
    "print(\"Formato de las observaciones:\")\n",
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channels_last\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "permute (Permute)            (None, 84, 84, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 84, 84, 32)        8224      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 84, 84, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 42, 42, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 42, 42, 64)        32832     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 42, 42, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 42, 42, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 42, 42, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 225792)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               28901504  \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 774       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 29,017,190\n",
      "Trainable params: 29,017,190\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Next, we build our model. We use the same model that was described by Mnih et al. (2015).\n",
    "input_shape = (WINDOW_LENGTH,) + INPUT_SHAPE\n",
    "model = Sequential()\n",
    "print(K.image_data_format())\n",
    "if K.image_data_format() == 'channels_last':\n",
    "    # (width, height, channels)\n",
    "    model.add(Permute((2, 3, 1), input_shape=input_shape))\n",
    "elif K.image_data_format() == 'channels_first':\n",
    "    # (channels, width, height)\n",
    "    model.add(Permute((1, 2, 3), input_shape=input_shape))\n",
    "else:\n",
    "    raise RuntimeError('Unknown image_dim_ordering.')\n",
    "# se podria sustituir el stride alto por capas de pooling\n",
    "\n",
    "\n",
    "model.add(Convolution2D(filters=32, kernel_size= (8,8), strides=(1,1), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "model.add(Convolution2D(filters= 64, kernel_size=(4,4), strides=(1,1), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(filters= 128, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Implementación de la solución DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit=1_000_000,             # memoria muy grande \n",
    "                          window_length=WINDOW_LENGTH) # 4\n",
    "processor = AtariProcessor()                           # creamos la instancia de preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(),  # definimos la policy\n",
    "                              attr='eps',          # pero con un scheduler\n",
    "                              value_max=1.,        # empiezaa en 1\n",
    "                              value_min=.05,       # termina en 0.1 en el ultimo step\n",
    "                              value_test=.05,      # en testeo será de 0.05\n",
    "                              nb_steps=1_800_000)  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQNAgent(model=model, \n",
    "               nb_actions=nb_actions, \n",
    "               policy=policy,\n",
    "               memory=memory, \n",
    "               processor=processor,\n",
    "               nb_steps_warmup=20_000,       # tenemos experience replay\n",
    "               gamma=.99,                    # el discount reward\n",
    "               target_model_update=10_000,   # cada 5000 itereaciones se actualiza se actualiza el modelo\n",
    "               train_interval=4)             # cada cuantas iteraciones se hace un step de entrenaminto\n",
    "\n",
    "dqn.compile(Adam(learning_rate=.00025), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training part\n",
    "weights_filename = 'dqn_{}_weights.h5f'.format(env_name)\n",
    "checkpoint_weights_filename = 'dqn_' + env_name + '_weights_{step}.h5f'\n",
    "log_filename = 'dqn_{}_log.json'.format(env_name)\n",
    "callbacks = [ModelIntervalCheckpoint(checkpoint_weights_filename, interval=50_000)]\n",
    "\n",
    "callbacks += [FileLogger(log_filename, interval=50_000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 2000000 steps ...\n",
      "Interval 1 (0 steps performed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\plane\\miniconda3\\envs\\venv_RL\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2424: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 63s 6ms/step - reward: 0.0124\n",
      "15 episodes - episode_reward: 8.200 [1.000, 21.000] - ale.lives: 2.160\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: 0.0134\n",
      "15 episodes - episode_reward: 8.467 [2.000, 14.000] - ale.lives: 2.126\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 345s 34ms/step - reward: 0.0157\n",
      "14 episodes - episode_reward: 11.500 [3.000, 26.000] - loss: 0.007 - mae: 0.027 - mean_q: 0.040 - mean_eps: 0.987 - ale.lives: 2.084\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 343s 34ms/step - reward: 0.0145\n",
      "12 episodes - episode_reward: 11.833 [4.000, 23.000] - loss: 0.007 - mae: 0.060 - mean_q: 0.083 - mean_eps: 0.982 - ale.lives: 2.054\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 342s 34ms/step - reward: 0.0121\n",
      "13 episodes - episode_reward: 9.769 [4.000, 26.000] - loss: 0.006 - mae: 0.089 - mean_q: 0.118 - mean_eps: 0.976 - ale.lives: 2.084\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 342s 34ms/step - reward: 0.0124\n",
      "15 episodes - episode_reward: 7.733 [3.000, 19.000] - loss: 0.006 - mae: 0.117 - mean_q: 0.150 - mean_eps: 0.971 - ale.lives: 2.113\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 358s 36ms/step - reward: 0.0133\n",
      "14 episodes - episode_reward: 9.000 [2.000, 17.000] - loss: 0.007 - mae: 0.154 - mean_q: 0.194 - mean_eps: 0.966 - ale.lives: 2.091\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 381s 38ms/step - reward: 0.0148\n",
      "14 episodes - episode_reward: 11.286 [3.000, 18.000] - loss: 0.007 - mae: 0.183 - mean_q: 0.226 - mean_eps: 0.960 - ale.lives: 2.009\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 348s 35ms/step - reward: 0.0151\n",
      "16 episodes - episode_reward: 9.562 [4.000, 20.000] - loss: 0.008 - mae: 0.191 - mean_q: 0.232 - mean_eps: 0.955 - ale.lives: 2.172\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 351s 35ms/step - reward: 0.0138\n",
      "12 episodes - episode_reward: 11.250 [4.000, 18.000] - loss: 0.008 - mae: 0.208 - mean_q: 0.251 - mean_eps: 0.950 - ale.lives: 1.982\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 312s 31ms/step - reward: 0.0140\n",
      "17 episodes - episode_reward: 8.471 [3.000, 19.000] - loss: 0.008 - mae: 0.224 - mean_q: 0.270 - mean_eps: 0.945 - ale.lives: 2.150\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 303s 30ms/step - reward: 0.0138\n",
      "15 episodes - episode_reward: 9.200 [2.000, 20.000] - loss: 0.009 - mae: 0.225 - mean_q: 0.269 - mean_eps: 0.939 - ale.lives: 2.208\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "10000/10000 [==============================] - 305s 30ms/step - reward: 0.0134\n",
      "16 episodes - episode_reward: 8.188 [4.000, 13.000] - loss: 0.011 - mae: 0.256 - mean_q: 0.305 - mean_eps: 0.934 - ale.lives: 2.104\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "10000/10000 [==============================] - 305s 30ms/step - reward: 0.0136\n",
      "16 episodes - episode_reward: 8.125 [4.000, 21.000] - loss: 0.012 - mae: 0.285 - mean_q: 0.339 - mean_eps: 0.929 - ale.lives: 2.048\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "10000/10000 [==============================] - 306s 31ms/step - reward: 0.0138\n",
      "13 episodes - episode_reward: 11.000 [4.000, 26.000] - loss: 0.011 - mae: 0.289 - mean_q: 0.342 - mean_eps: 0.923 - ale.lives: 2.029\n",
      "\n",
      "Interval 16 (150000 steps performed)\n",
      "10000/10000 [==============================] - 308s 31ms/step - reward: 0.0143\n",
      "16 episodes - episode_reward: 9.375 [4.000, 19.000] - loss: 0.012 - mae: 0.307 - mean_q: 0.364 - mean_eps: 0.918 - ale.lives: 2.149\n",
      "\n",
      "Interval 17 (160000 steps performed)\n",
      "10000/10000 [==============================] - 308s 31ms/step - reward: 0.0154\n",
      "15 episodes - episode_reward: 10.267 [6.000, 17.000] - loss: 0.012 - mae: 0.330 - mean_q: 0.392 - mean_eps: 0.913 - ale.lives: 2.118\n",
      "\n",
      "Interval 18 (170000 steps performed)\n",
      "10000/10000 [==============================] - 308s 31ms/step - reward: 0.0152\n",
      "13 episodes - episode_reward: 10.000 [5.000, 17.000] - loss: 0.012 - mae: 0.348 - mean_q: 0.415 - mean_eps: 0.908 - ale.lives: 2.194\n",
      "\n",
      "Interval 19 (180000 steps performed)\n",
      "10000/10000 [==============================] - 310s 31ms/step - reward: 0.0158\n",
      "16 episodes - episode_reward: 10.938 [2.000, 22.000] - loss: 0.011 - mae: 0.365 - mean_q: 0.435 - mean_eps: 0.902 - ale.lives: 2.078\n",
      "\n",
      "Interval 20 (190000 steps performed)\n",
      "10000/10000 [==============================] - 312s 31ms/step - reward: 0.0145\n",
      "15 episodes - episode_reward: 9.400 [4.000, 22.000] - loss: 0.011 - mae: 0.390 - mean_q: 0.463 - mean_eps: 0.897 - ale.lives: 2.018\n",
      "\n",
      "Interval 21 (200000 steps performed)\n",
      "10000/10000 [==============================] - 314s 31ms/step - reward: 0.0151\n",
      "14 episodes - episode_reward: 11.214 [6.000, 22.000] - loss: 0.011 - mae: 0.417 - mean_q: 0.496 - mean_eps: 0.892 - ale.lives: 2.093\n",
      "\n",
      "Interval 22 (210000 steps performed)\n",
      "10000/10000 [==============================] - 314s 31ms/step - reward: 0.0153\n",
      "15 episodes - episode_reward: 9.733 [5.000, 20.000] - loss: 0.011 - mae: 0.428 - mean_q: 0.509 - mean_eps: 0.887 - ale.lives: 2.158\n",
      "\n",
      "Interval 23 (220000 steps performed)\n",
      "10000/10000 [==============================] - 344s 34ms/step - reward: 0.0163\n",
      "15 episodes - episode_reward: 11.000 [4.000, 25.000] - loss: 0.010 - mae: 0.441 - mean_q: 0.526 - mean_eps: 0.881 - ale.lives: 2.247\n",
      "\n",
      "Interval 24 (230000 steps performed)\n",
      "10000/10000 [==============================] - 514s 51ms/step - reward: 0.0153\n",
      "14 episodes - episode_reward: 11.500 [3.000, 21.000] - loss: 0.011 - mae: 0.466 - mean_q: 0.555 - mean_eps: 0.876 - ale.lives: 2.132\n",
      "\n",
      "Interval 25 (240000 steps performed)\n",
      "10000/10000 [==============================] - 319s 32ms/step - reward: 0.0123\n",
      "15 episodes - episode_reward: 7.800 [2.000, 13.000] - loss: 0.011 - mae: 0.483 - mean_q: 0.577 - mean_eps: 0.871 - ale.lives: 2.070\n",
      "\n",
      "Interval 26 (250000 steps performed)\n",
      "10000/10000 [==============================] - 317s 32ms/step - reward: 0.0139\n",
      "14 episodes - episode_reward: 10.214 [3.000, 24.000] - loss: 0.012 - mae: 0.500 - mean_q: 0.597 - mean_eps: 0.865 - ale.lives: 2.134\n",
      "\n",
      "Interval 27 (260000 steps performed)\n",
      "10000/10000 [==============================] - 310s 31ms/step - reward: 0.0149\n",
      "15 episodes - episode_reward: 9.800 [5.000, 18.000] - loss: 0.013 - mae: 0.522 - mean_q: 0.626 - mean_eps: 0.860 - ale.lives: 2.069\n",
      "\n",
      "Interval 28 (270000 steps performed)\n",
      "10000/10000 [==============================] - 315s 31ms/step - reward: 0.0141\n",
      "13 episodes - episode_reward: 10.308 [5.000, 19.000] - loss: 0.012 - mae: 0.560 - mean_q: 0.670 - mean_eps: 0.855 - ale.lives: 2.239\n",
      "\n",
      "Interval 29 (280000 steps performed)\n",
      "10000/10000 [==============================] - 313s 31ms/step - reward: 0.0132\n",
      "15 episodes - episode_reward: 9.200 [4.000, 15.000] - loss: 0.013 - mae: 0.575 - mean_q: 0.685 - mean_eps: 0.850 - ale.lives: 2.132\n",
      "\n",
      "Interval 30 (290000 steps performed)\n",
      "10000/10000 [==============================] - 315s 31ms/step - reward: 0.0141\n",
      "15 episodes - episode_reward: 9.267 [4.000, 21.000] - loss: 0.012 - mae: 0.581 - mean_q: 0.694 - mean_eps: 0.844 - ale.lives: 2.109\n",
      "\n",
      "Interval 31 (300000 steps performed)\n",
      "10000/10000 [==============================] - 315s 32ms/step - reward: 0.0151\n",
      "16 episodes - episode_reward: 9.250 [2.000, 21.000] - loss: 0.012 - mae: 0.589 - mean_q: 0.704 - mean_eps: 0.839 - ale.lives: 2.225\n",
      "\n",
      "Interval 32 (310000 steps performed)\n",
      "10000/10000 [==============================] - 316s 32ms/step - reward: 0.0145\n",
      "13 episodes - episode_reward: 11.462 [4.000, 24.000] - loss: 0.011 - mae: 0.603 - mean_q: 0.721 - mean_eps: 0.834 - ale.lives: 2.223\n",
      "\n",
      "Interval 33 (320000 steps performed)\n",
      "10000/10000 [==============================] - 319s 32ms/step - reward: 0.0171\n",
      "13 episodes - episode_reward: 12.308 [6.000, 28.000] - loss: 0.012 - mae: 0.616 - mean_q: 0.736 - mean_eps: 0.828 - ale.lives: 2.089\n",
      "\n",
      "Interval 34 (330000 steps performed)\n",
      "10000/10000 [==============================] - 318s 32ms/step - reward: 0.0155\n",
      "13 episodes - episode_reward: 13.231 [3.000, 30.000] - loss: 0.013 - mae: 0.656 - mean_q: 0.785 - mean_eps: 0.823 - ale.lives: 1.973\n",
      "\n",
      "Interval 35 (340000 steps performed)\n",
      "10000/10000 [==============================] - 319s 32ms/step - reward: 0.0134\n",
      "14 episodes - episode_reward: 9.143 [4.000, 15.000] - loss: 0.013 - mae: 0.675 - mean_q: 0.806 - mean_eps: 0.818 - ale.lives: 2.176\n",
      "\n",
      "Interval 36 (350000 steps performed)\n",
      "10000/10000 [==============================] - 320s 32ms/step - reward: 0.0137\n",
      "13 episodes - episode_reward: 10.077 [4.000, 19.000] - loss: 0.013 - mae: 0.672 - mean_q: 0.805 - mean_eps: 0.813 - ale.lives: 2.125\n",
      "\n",
      "Interval 37 (360000 steps performed)\n",
      "10000/10000 [==============================] - 321s 32ms/step - reward: 0.0158\n",
      "13 episodes - episode_reward: 13.077 [3.000, 28.000] - loss: 0.016 - mae: 0.683 - mean_q: 0.818 - mean_eps: 0.807 - ale.lives: 2.100\n",
      "\n",
      "Interval 38 (370000 steps performed)\n",
      "10000/10000 [==============================] - 325s 32ms/step - reward: 0.0174\n",
      "13 episodes - episode_reward: 12.769 [6.000, 25.000] - loss: 0.015 - mae: 0.713 - mean_q: 0.858 - mean_eps: 0.802 - ale.lives: 1.975\n",
      "\n",
      "Interval 39 (380000 steps performed)\n",
      "10000/10000 [==============================] - 329s 33ms/step - reward: 0.0163\n",
      "14 episodes - episode_reward: 11.286 [4.000, 32.000] - loss: 0.014 - mae: 0.732 - mean_q: 0.876 - mean_eps: 0.797 - ale.lives: 2.058\n",
      "\n",
      "Interval 40 (390000 steps performed)\n",
      "10000/10000 [==============================] - 330s 33ms/step - reward: 0.0137\n",
      "16 episodes - episode_reward: 8.938 [4.000, 21.000] - loss: 0.011 - mae: 0.742 - mean_q: 0.888 - mean_eps: 0.792 - ale.lives: 1.949\n",
      "\n",
      "Interval 41 (400000 steps performed)\n",
      "10000/10000 [==============================] - 333s 33ms/step - reward: 0.0168\n",
      "13 episodes - episode_reward: 12.538 [3.000, 23.000] - loss: 0.011 - mae: 0.754 - mean_q: 0.902 - mean_eps: 0.786 - ale.lives: 1.994\n",
      "\n",
      "Interval 42 (410000 steps performed)\n",
      "10000/10000 [==============================] - 331s 33ms/step - reward: 0.0156\n",
      "15 episodes - episode_reward: 10.867 [4.000, 19.000] - loss: 0.012 - mae: 0.773 - mean_q: 0.925 - mean_eps: 0.781 - ale.lives: 2.140\n",
      "\n",
      "Interval 43 (420000 steps performed)\n",
      "10000/10000 [==============================] - 331s 33ms/step - reward: 0.0139\n",
      "14 episodes - episode_reward: 10.143 [6.000, 18.000] - loss: 0.011 - mae: 0.776 - mean_q: 0.930 - mean_eps: 0.776 - ale.lives: 2.040\n",
      "\n",
      "Interval 44 (430000 steps performed)\n",
      "10000/10000 [==============================] - 317s 32ms/step - reward: 0.0171\n",
      "14 episodes - episode_reward: 12.000 [4.000, 24.000] - loss: 0.011 - mae: 0.778 - mean_q: 0.932 - mean_eps: 0.770 - ale.lives: 2.108\n",
      "\n",
      "Interval 45 (440000 steps performed)\n",
      "10000/10000 [==============================] - 328s 33ms/step - reward: 0.0147\n",
      "15 episodes - episode_reward: 10.133 [2.000, 21.000] - loss: 0.011 - mae: 0.766 - mean_q: 0.918 - mean_eps: 0.765 - ale.lives: 2.132\n",
      "\n",
      "Interval 46 (450000 steps performed)\n",
      "10000/10000 [==============================] - 510s 51ms/step - reward: 0.0152\n",
      "12 episodes - episode_reward: 11.833 [3.000, 20.000] - loss: 0.012 - mae: 0.774 - mean_q: 0.928 - mean_eps: 0.760 - ale.lives: 2.177\n",
      "\n",
      "Interval 47 (460000 steps performed)\n",
      "10000/10000 [==============================] - 439s 44ms/step - reward: 0.0165\n",
      "16 episodes - episode_reward: 10.625 [2.000, 21.000] - loss: 0.011 - mae: 0.780 - mean_q: 0.935 - mean_eps: 0.755 - ale.lives: 2.107\n",
      "\n",
      "Interval 48 (470000 steps performed)\n",
      "10000/10000 [==============================] - 438s 44ms/step - reward: 0.0138\n",
      "15 episodes - episode_reward: 9.267 [5.000, 17.000] - loss: 0.011 - mae: 0.796 - mean_q: 0.958 - mean_eps: 0.749 - ale.lives: 2.132\n",
      "\n",
      "Interval 49 (480000 steps performed)\n",
      "10000/10000 [==============================] - 445s 44ms/step - reward: 0.0150\n",
      "14 episodes - episode_reward: 10.500 [4.000, 19.000] - loss: 0.010 - mae: 0.807 - mean_q: 0.968 - mean_eps: 0.744 - ale.lives: 1.961\n",
      "\n",
      "Interval 50 (490000 steps performed)\n",
      "10000/10000 [==============================] - 450s 45ms/step - reward: 0.0135\n",
      "14 episodes - episode_reward: 9.929 [3.000, 18.000] - loss: 0.011 - mae: 0.825 - mean_q: 0.987 - mean_eps: 0.739 - ale.lives: 2.089\n",
      "\n",
      "Interval 51 (500000 steps performed)\n",
      "10000/10000 [==============================] - 452s 45ms/step - reward: 0.0154\n",
      "12 episodes - episode_reward: 12.417 [3.000, 31.000] - loss: 0.013 - mae: 0.846 - mean_q: 1.014 - mean_eps: 0.733 - ale.lives: 2.124\n",
      "\n",
      "Interval 52 (510000 steps performed)\n",
      "10000/10000 [==============================] - 444s 44ms/step - reward: 0.0149\n",
      "15 episodes - episode_reward: 9.333 [4.000, 22.000] - loss: 0.012 - mae: 0.849 - mean_q: 1.018 - mean_eps: 0.728 - ale.lives: 2.067\n",
      "\n",
      "Interval 53 (520000 steps performed)\n",
      "10000/10000 [==============================] - 388s 39ms/step - reward: 0.0162\n",
      "16 episodes - episode_reward: 11.000 [5.000, 23.000] - loss: 0.012 - mae: 0.855 - mean_q: 1.024 - mean_eps: 0.723 - ale.lives: 2.081\n",
      "\n",
      "Interval 54 (530000 steps performed)\n",
      "10000/10000 [==============================] - 366s 37ms/step - reward: 0.0148\n",
      "15 episodes - episode_reward: 9.867 [4.000, 20.000] - loss: 0.011 - mae: 0.872 - mean_q: 1.043 - mean_eps: 0.718 - ale.lives: 2.056\n",
      "\n",
      "Interval 55 (540000 steps performed)\n",
      "10000/10000 [==============================] - 357s 36ms/step - reward: 0.0170\n",
      "14 episodes - episode_reward: 11.500 [4.000, 24.000] - loss: 0.013 - mae: 0.878 - mean_q: 1.053 - mean_eps: 0.712 - ale.lives: 2.071\n",
      "\n",
      "Interval 56 (550000 steps performed)\n",
      "10000/10000 [==============================] - 359s 36ms/step - reward: 0.0184\n",
      "14 episodes - episode_reward: 13.643 [4.000, 35.000] - loss: 0.012 - mae: 0.884 - mean_q: 1.060 - mean_eps: 0.707 - ale.lives: 1.975\n",
      "\n",
      "Interval 57 (560000 steps performed)\n",
      "10000/10000 [==============================] - 361s 36ms/step - reward: 0.0166\n",
      "15 episodes - episode_reward: 11.400 [2.000, 26.000] - loss: 0.012 - mae: 0.908 - mean_q: 1.087 - mean_eps: 0.702 - ale.lives: 2.051\n",
      "\n",
      "Interval 58 (570000 steps performed)\n",
      "10000/10000 [==============================] - 369s 37ms/step - reward: 0.0177\n",
      "12 episodes - episode_reward: 14.250 [6.000, 25.000] - loss: 0.012 - mae: 0.908 - mean_q: 1.087 - mean_eps: 0.697 - ale.lives: 2.139\n",
      "\n",
      "Interval 59 (580000 steps performed)\n",
      "10000/10000 [==============================] - 370s 37ms/step - reward: 0.0165\n",
      "15 episodes - episode_reward: 11.400 [5.000, 20.000] - loss: 0.012 - mae: 0.926 - mean_q: 1.109 - mean_eps: 0.691 - ale.lives: 2.000\n",
      "\n",
      "Interval 60 (590000 steps performed)\n",
      "10000/10000 [==============================] - 378s 38ms/step - reward: 0.0169\n",
      "14 episodes - episode_reward: 11.857 [3.000, 22.000] - loss: 0.011 - mae: 0.933 - mean_q: 1.118 - mean_eps: 0.686 - ale.lives: 2.015\n",
      "\n",
      "Interval 61 (600000 steps performed)\n",
      "10000/10000 [==============================] - 386s 39ms/step - reward: 0.0191\n",
      "13 episodes - episode_reward: 14.385 [3.000, 23.000] - loss: 0.012 - mae: 0.943 - mean_q: 1.130 - mean_eps: 0.681 - ale.lives: 2.079\n",
      "\n",
      "Interval 62 (610000 steps performed)\n",
      "10000/10000 [==============================] - 376s 38ms/step - reward: 0.0165\n",
      "11 episodes - episode_reward: 14.091 [7.000, 31.000] - loss: 0.012 - mae: 0.954 - mean_q: 1.144 - mean_eps: 0.675 - ale.lives: 1.939\n",
      "\n",
      "Interval 63 (620000 steps performed)\n",
      "10000/10000 [==============================] - 368s 37ms/step - reward: 0.0146\n",
      "16 episodes - episode_reward: 10.000 [3.000, 21.000] - loss: 0.011 - mae: 0.972 - mean_q: 1.165 - mean_eps: 0.670 - ale.lives: 2.001\n",
      "\n",
      "Interval 64 (630000 steps performed)\n",
      "10000/10000 [==============================] - 390s 39ms/step - reward: 0.0156\n",
      "14 episodes - episode_reward: 11.071 [4.000, 25.000] - loss: 0.011 - mae: 0.983 - mean_q: 1.177 - mean_eps: 0.665 - ale.lives: 2.129\n",
      "\n",
      "Interval 65 (640000 steps performed)\n",
      "10000/10000 [==============================] - 389s 39ms/step - reward: 0.0195\n",
      "14 episodes - episode_reward: 14.214 [8.000, 34.000] - loss: 0.011 - mae: 1.007 - mean_q: 1.207 - mean_eps: 0.660 - ale.lives: 2.100\n",
      "\n",
      "Interval 66 (650000 steps performed)\n",
      "10000/10000 [==============================] - 382s 38ms/step - reward: 0.0186\n",
      "12 episodes - episode_reward: 15.333 [5.000, 29.000] - loss: 0.013 - mae: 1.007 - mean_q: 1.207 - mean_eps: 0.654 - ale.lives: 1.939\n",
      "\n",
      "Interval 67 (660000 steps performed)\n",
      "10000/10000 [==============================] - 386s 39ms/step - reward: 0.0183\n",
      "12 episodes - episode_reward: 15.083 [7.000, 24.000] - loss: 0.011 - mae: 1.034 - mean_q: 1.239 - mean_eps: 0.649 - ale.lives: 2.054\n",
      "\n",
      "Interval 68 (670000 steps performed)\n",
      "10000/10000 [==============================] - 388s 39ms/step - reward: 0.0179\n",
      "13 episodes - episode_reward: 13.846 [6.000, 26.000] - loss: 0.012 - mae: 1.054 - mean_q: 1.263 - mean_eps: 0.644 - ale.lives: 1.966\n",
      "\n",
      "Interval 69 (680000 steps performed)\n",
      "10000/10000 [==============================] - 389s 39ms/step - reward: 0.0171\n",
      "13 episodes - episode_reward: 12.846 [4.000, 25.000] - loss: 0.012 - mae: 1.084 - mean_q: 1.298 - mean_eps: 0.638 - ale.lives: 2.029\n",
      "\n",
      "Interval 70 (690000 steps performed)\n",
      "10000/10000 [==============================] - 407s 41ms/step - reward: 0.0199\n",
      "14 episodes - episode_reward: 14.286 [4.000, 25.000] - loss: 0.012 - mae: 1.103 - mean_q: 1.322 - mean_eps: 0.633 - ale.lives: 2.024\n",
      "\n",
      "Interval 71 (700000 steps performed)\n",
      "10000/10000 [==============================] - 400s 40ms/step - reward: 0.0172\n",
      "15 episodes - episode_reward: 11.133 [4.000, 20.000] - loss: 0.014 - mae: 1.102 - mean_q: 1.321 - mean_eps: 0.628 - ale.lives: 2.003\n",
      "\n",
      "Interval 72 (710000 steps performed)\n",
      "10000/10000 [==============================] - 383s 38ms/step - reward: 0.0181\n",
      "17 episodes - episode_reward: 11.294 [1.000, 32.000] - loss: 0.013 - mae: 1.121 - mean_q: 1.344 - mean_eps: 0.623 - ale.lives: 1.948\n",
      "\n",
      "Interval 73 (720000 steps performed)\n",
      "10000/10000 [==============================] - 338s 34ms/step - reward: 0.0166\n",
      "14 episodes - episode_reward: 11.857 [3.000, 27.000] - loss: 0.012 - mae: 1.125 - mean_q: 1.348 - mean_eps: 0.617 - ale.lives: 2.050\n",
      "\n",
      "Interval 74 (730000 steps performed)\n",
      "10000/10000 [==============================] - 351s 35ms/step - reward: 0.0190\n",
      "12 episodes - episode_reward: 14.750 [6.000, 25.000] - loss: 0.013 - mae: 1.132 - mean_q: 1.356 - mean_eps: 0.612 - ale.lives: 2.136\n",
      "\n",
      "Interval 75 (740000 steps performed)\n",
      "10000/10000 [==============================] - 346s 35ms/step - reward: 0.0201\n",
      "14 episodes - episode_reward: 15.143 [3.000, 28.000] - loss: 0.012 - mae: 1.122 - mean_q: 1.345 - mean_eps: 0.607 - ale.lives: 2.058\n",
      "\n",
      "Interval 76 (750000 steps performed)\n",
      "10000/10000 [==============================] - 354s 35ms/step - reward: 0.0202\n",
      "12 episodes - episode_reward: 15.750 [6.000, 24.000] - loss: 0.013 - mae: 1.145 - mean_q: 1.374 - mean_eps: 0.602 - ale.lives: 1.908\n",
      "\n",
      "Interval 77 (760000 steps performed)\n",
      "10000/10000 [==============================] - 367s 37ms/step - reward: 0.0182\n",
      "16 episodes - episode_reward: 12.188 [3.000, 26.000] - loss: 0.011 - mae: 1.121 - mean_q: 1.346 - mean_eps: 0.596 - ale.lives: 1.933\n",
      "\n",
      "Interval 78 (770000 steps performed)\n",
      "10000/10000 [==============================] - 371s 37ms/step - reward: 0.0203\n",
      "13 episodes - episode_reward: 15.077 [8.000, 24.000] - loss: 0.012 - mae: 1.120 - mean_q: 1.344 - mean_eps: 0.591 - ale.lives: 1.981\n",
      "\n",
      "Interval 79 (780000 steps performed)\n",
      "10000/10000 [==============================] - 369s 37ms/step - reward: 0.0186\n",
      "14 episodes - episode_reward: 13.143 [3.000, 21.000] - loss: 0.012 - mae: 1.134 - mean_q: 1.360 - mean_eps: 0.586 - ale.lives: 2.047\n",
      "\n",
      "Interval 80 (790000 steps performed)\n",
      "10000/10000 [==============================] - 373s 37ms/step - reward: 0.0204\n",
      "13 episodes - episode_reward: 16.077 [7.000, 35.000] - loss: 0.011 - mae: 1.141 - mean_q: 1.368 - mean_eps: 0.580 - ale.lives: 1.984\n",
      "\n",
      "Interval 81 (800000 steps performed)\n",
      "10000/10000 [==============================] - 370s 37ms/step - reward: 0.0164\n",
      "14 episodes - episode_reward: 11.500 [5.000, 18.000] - loss: 0.012 - mae: 1.153 - mean_q: 1.382 - mean_eps: 0.575 - ale.lives: 2.053\n",
      "\n",
      "Interval 82 (810000 steps performed)\n",
      "10000/10000 [==============================] - 374s 37ms/step - reward: 0.0180\n",
      "13 episodes - episode_reward: 12.923 [4.000, 22.000] - loss: 0.011 - mae: 1.155 - mean_q: 1.384 - mean_eps: 0.570 - ale.lives: 1.983\n",
      "\n",
      "Interval 83 (820000 steps performed)\n",
      "10000/10000 [==============================] - 377s 38ms/step - reward: 0.0189\n",
      "15 episodes - episode_reward: 13.400 [5.000, 24.000] - loss: 0.012 - mae: 1.171 - mean_q: 1.403 - mean_eps: 0.565 - ale.lives: 2.142\n",
      "\n",
      "Interval 84 (830000 steps performed)\n",
      "10000/10000 [==============================] - 375s 37ms/step - reward: 0.0165\n",
      "11 episodes - episode_reward: 14.364 [5.000, 23.000] - loss: 0.012 - mae: 1.179 - mean_q: 1.413 - mean_eps: 0.559 - ale.lives: 1.895\n",
      "\n",
      "Interval 85 (840000 steps performed)\n",
      "10000/10000 [==============================] - 381s 38ms/step - reward: 0.0190\n",
      "13 episodes - episode_reward: 15.231 [3.000, 23.000] - loss: 0.012 - mae: 1.192 - mean_q: 1.430 - mean_eps: 0.554 - ale.lives: 2.041\n",
      "\n",
      "Interval 86 (850000 steps performed)\n",
      "10000/10000 [==============================] - 385s 38ms/step - reward: 0.0200\n",
      "13 episodes - episode_reward: 15.385 [8.000, 31.000] - loss: 0.012 - mae: 1.211 - mean_q: 1.452 - mean_eps: 0.549 - ale.lives: 2.090\n",
      "\n",
      "Interval 87 (860000 steps performed)\n",
      "10000/10000 [==============================] - 381s 38ms/step - reward: 0.0187\n",
      "15 episodes - episode_reward: 12.667 [3.000, 25.000] - loss: 0.012 - mae: 1.216 - mean_q: 1.458 - mean_eps: 0.543 - ale.lives: 1.972\n",
      "\n",
      "Interval 88 (870000 steps performed)\n",
      "10000/10000 [==============================] - 384s 38ms/step - reward: 0.0177\n",
      "12 episodes - episode_reward: 14.750 [6.000, 24.000] - loss: 0.011 - mae: 1.232 - mean_q: 1.478 - mean_eps: 0.538 - ale.lives: 1.967\n",
      "\n",
      "Interval 89 (880000 steps performed)\n",
      "10000/10000 [==============================] - 387s 39ms/step - reward: 0.0200\n",
      "16 episodes - episode_reward: 12.312 [3.000, 17.000] - loss: 0.012 - mae: 1.231 - mean_q: 1.476 - mean_eps: 0.533 - ale.lives: 2.100\n",
      "\n",
      "Interval 90 (890000 steps performed)\n",
      "10000/10000 [==============================] - 392s 39ms/step - reward: 0.0190\n",
      "12 episodes - episode_reward: 16.500 [6.000, 23.000] - loss: 0.012 - mae: 1.251 - mean_q: 1.500 - mean_eps: 0.528 - ale.lives: 1.944\n",
      "\n",
      "Interval 91 (900000 steps performed)\n",
      "10000/10000 [==============================] - 394s 39ms/step - reward: 0.0187\n",
      "13 episodes - episode_reward: 13.846 [7.000, 22.000] - loss: 0.013 - mae: 1.254 - mean_q: 1.503 - mean_eps: 0.522 - ale.lives: 1.932\n",
      "\n",
      "Interval 92 (910000 steps performed)\n",
      "10000/10000 [==============================] - 395s 40ms/step - reward: 0.0182\n",
      "12 episodes - episode_reward: 14.417 [7.000, 24.000] - loss: 0.013 - mae: 1.277 - mean_q: 1.531 - mean_eps: 0.517 - ale.lives: 2.034\n",
      "\n",
      "Interval 93 (920000 steps performed)\n",
      "10000/10000 [==============================] - 394s 39ms/step - reward: 0.0198\n",
      "14 episodes - episode_reward: 14.786 [7.000, 26.000] - loss: 0.013 - mae: 1.287 - mean_q: 1.544 - mean_eps: 0.512 - ale.lives: 1.994\n",
      "\n",
      "Interval 94 (930000 steps performed)\n",
      "10000/10000 [==============================] - 396s 40ms/step - reward: 0.0174\n",
      "14 episodes - episode_reward: 12.857 [7.000, 22.000] - loss: 0.014 - mae: 1.303 - mean_q: 1.562 - mean_eps: 0.507 - ale.lives: 2.175\n",
      "\n",
      "Interval 95 (940000 steps performed)\n",
      "10000/10000 [==============================] - 400s 40ms/step - reward: 0.0177\n",
      "12 episodes - episode_reward: 14.250 [7.000, 25.000] - loss: 0.012 - mae: 1.307 - mean_q: 1.566 - mean_eps: 0.501 - ale.lives: 2.084\n",
      "\n",
      "Interval 96 (950000 steps performed)\n",
      "10000/10000 [==============================] - 401s 40ms/step - reward: 0.0169\n",
      "12 episodes - episode_reward: 14.000 [6.000, 20.000] - loss: 0.013 - mae: 1.312 - mean_q: 1.573 - mean_eps: 0.496 - ale.lives: 1.988\n",
      "\n",
      "Interval 97 (960000 steps performed)\n",
      "10000/10000 [==============================] - 405s 40ms/step - reward: 0.0198\n",
      "14 episodes - episode_reward: 14.357 [7.000, 22.000] - loss: 0.014 - mae: 1.330 - mean_q: 1.594 - mean_eps: 0.491 - ale.lives: 2.052\n",
      "\n",
      "Interval 98 (970000 steps performed)\n",
      "10000/10000 [==============================] - 407s 41ms/step - reward: 0.0162\n",
      "11 episodes - episode_reward: 15.182 [5.000, 27.000] - loss: 0.014 - mae: 1.337 - mean_q: 1.602 - mean_eps: 0.485 - ale.lives: 1.911\n",
      "\n",
      "Interval 99 (980000 steps performed)\n",
      "10000/10000 [==============================] - 410s 41ms/step - reward: 0.0180\n",
      "14 episodes - episode_reward: 12.786 [6.000, 23.000] - loss: 0.014 - mae: 1.357 - mean_q: 1.627 - mean_eps: 0.480 - ale.lives: 2.220\n",
      "\n",
      "Interval 100 (990000 steps performed)\n",
      "10000/10000 [==============================] - 415s 41ms/step - reward: 0.0187\n",
      "13 episodes - episode_reward: 14.462 [6.000, 22.000] - loss: 0.014 - mae: 1.368 - mean_q: 1.640 - mean_eps: 0.475 - ale.lives: 1.990\n",
      "\n",
      "Interval 101 (1000000 steps performed)\n",
      "10000/10000 [==============================] - 416s 42ms/step - reward: 0.0178\n",
      "14 episodes - episode_reward: 12.429 [5.000, 22.000] - loss: 0.014 - mae: 1.378 - mean_q: 1.650 - mean_eps: 0.470 - ale.lives: 2.149\n",
      "\n",
      "Interval 102 (1010000 steps performed)\n",
      "10000/10000 [==============================] - 410s 41ms/step - reward: 0.0177\n",
      "13 episodes - episode_reward: 13.615 [7.000, 21.000] - loss: 0.014 - mae: 1.395 - mean_q: 1.672 - mean_eps: 0.464 - ale.lives: 2.067\n",
      "\n",
      "Interval 103 (1020000 steps performed)\n",
      "10000/10000 [==============================] - 406s 41ms/step - reward: 0.0188\n",
      "13 episodes - episode_reward: 14.769 [6.000, 30.000] - loss: 0.016 - mae: 1.391 - mean_q: 1.667 - mean_eps: 0.459 - ale.lives: 2.011\n",
      "\n",
      "Interval 104 (1030000 steps performed)\n",
      "10000/10000 [==============================] - 410s 41ms/step - reward: 0.0144\n",
      "13 episodes - episode_reward: 10.923 [5.000, 17.000] - loss: 0.016 - mae: 1.393 - mean_q: 1.670 - mean_eps: 0.454 - ale.lives: 1.927\n",
      "\n",
      "Interval 105 (1040000 steps performed)\n",
      "10000/10000 [==============================] - 415s 41ms/step - reward: 0.0141\n",
      "12 episodes - episode_reward: 11.417 [6.000, 17.000] - loss: 0.016 - mae: 1.419 - mean_q: 1.700 - mean_eps: 0.448 - ale.lives: 1.936\n",
      "\n",
      "Interval 106 (1050000 steps performed)\n",
      "10000/10000 [==============================] - 411s 41ms/step - reward: 0.0184\n",
      "12 episodes - episode_reward: 15.250 [9.000, 23.000] - loss: 0.015 - mae: 1.410 - mean_q: 1.688 - mean_eps: 0.443 - ale.lives: 1.968\n",
      "\n",
      "Interval 107 (1060000 steps performed)\n",
      "10000/10000 [==============================] - 410s 41ms/step - reward: 0.0155\n",
      "13 episodes - episode_reward: 12.385 [7.000, 20.000] - loss: 0.015 - mae: 1.410 - mean_q: 1.689 - mean_eps: 0.438 - ale.lives: 2.178\n",
      "\n",
      "Interval 108 (1070000 steps performed)\n",
      "10000/10000 [==============================] - 414s 41ms/step - reward: 0.0167\n",
      "14 episodes - episode_reward: 11.000 [6.000, 21.000] - loss: 0.015 - mae: 1.418 - mean_q: 1.699 - mean_eps: 0.433 - ale.lives: 2.124\n",
      "\n",
      "Interval 109 (1080000 steps performed)\n",
      "10000/10000 [==============================] - 410s 41ms/step - reward: 0.0167\n",
      "15 episodes - episode_reward: 11.533 [4.000, 30.000] - loss: 0.014 - mae: 1.430 - mean_q: 1.714 - mean_eps: 0.427 - ale.lives: 2.088\n",
      "\n",
      "Interval 110 (1090000 steps performed)\n",
      "10000/10000 [==============================] - 413s 41ms/step - reward: 0.0146\n",
      "12 episodes - episode_reward: 12.667 [5.000, 32.000] - loss: 0.013 - mae: 1.422 - mean_q: 1.703 - mean_eps: 0.422 - ale.lives: 1.993\n",
      "\n",
      "Interval 111 (1100000 steps performed)\n",
      "10000/10000 [==============================] - 412s 41ms/step - reward: 0.0172\n",
      "12 episodes - episode_reward: 14.500 [3.000, 27.000] - loss: 0.014 - mae: 1.426 - mean_q: 1.709 - mean_eps: 0.417 - ale.lives: 1.933\n",
      "\n",
      "Interval 112 (1110000 steps performed)\n",
      "10000/10000 [==============================] - 414s 41ms/step - reward: 0.0173\n",
      "11 episodes - episode_reward: 14.455 [9.000, 20.000] - loss: 0.014 - mae: 1.436 - mean_q: 1.720 - mean_eps: 0.412 - ale.lives: 2.015\n",
      "\n",
      "Interval 113 (1120000 steps performed)\n",
      "10000/10000 [==============================] - 410s 41ms/step - reward: 0.0172\n",
      "11 episodes - episode_reward: 16.273 [8.000, 29.000] - loss: 0.015 - mae: 1.446 - mean_q: 1.732 - mean_eps: 0.406 - ale.lives: 1.906\n",
      "\n",
      "Interval 114 (1130000 steps performed)\n",
      "10000/10000 [==============================] - 413s 41ms/step - reward: 0.0174\n",
      "13 episodes - episode_reward: 12.538 [4.000, 20.000] - loss: 0.014 - mae: 1.447 - mean_q: 1.734 - mean_eps: 0.401 - ale.lives: 2.120\n",
      "\n",
      "Interval 115 (1140000 steps performed)\n",
      "10000/10000 [==============================] - 412s 41ms/step - reward: 0.0190\n",
      "15 episodes - episode_reward: 13.333 [6.000, 23.000] - loss: 0.014 - mae: 1.468 - mean_q: 1.758 - mean_eps: 0.396 - ale.lives: 2.008\n",
      "\n",
      "Interval 116 (1150000 steps performed)\n",
      "10000/10000 [==============================] - 416s 42ms/step - reward: 0.0169\n",
      "14 episodes - episode_reward: 12.643 [5.000, 25.000] - loss: 0.015 - mae: 1.480 - mean_q: 1.774 - mean_eps: 0.390 - ale.lives: 2.091\n",
      "\n",
      "Interval 117 (1160000 steps performed)\n",
      "10000/10000 [==============================] - 409s 41ms/step - reward: 0.0169\n",
      "15 episodes - episode_reward: 10.867 [6.000, 19.000] - loss: 0.015 - mae: 1.506 - mean_q: 1.804 - mean_eps: 0.385 - ale.lives: 2.099\n",
      "\n",
      "Interval 118 (1170000 steps performed)\n",
      "10000/10000 [==============================] - 363s 36ms/step - reward: 0.0188\n",
      "13 episodes - episode_reward: 14.077 [5.000, 23.000] - loss: 0.014 - mae: 1.522 - mean_q: 1.824 - mean_eps: 0.380 - ale.lives: 2.087\n",
      "\n",
      "Interval 119 (1180000 steps performed)\n",
      "10000/10000 [==============================] - 369s 37ms/step - reward: 0.0166\n",
      "13 episodes - episode_reward: 13.231 [6.000, 22.000] - loss: 0.015 - mae: 1.519 - mean_q: 1.820 - mean_eps: 0.375 - ale.lives: 2.120\n",
      "\n",
      "Interval 120 (1190000 steps performed)\n",
      "10000/10000 [==============================] - 380s 38ms/step - reward: 0.0175\n",
      "14 episodes - episode_reward: 12.714 [6.000, 21.000] - loss: 0.015 - mae: 1.530 - mean_q: 1.834 - mean_eps: 0.369 - ale.lives: 2.006\n",
      "\n",
      "Interval 121 (1200000 steps performed)\n",
      "10000/10000 [==============================] - 754s 75ms/step - reward: 0.0167\n",
      "12 episodes - episode_reward: 13.667 [5.000, 25.000] - loss: 0.014 - mae: 1.536 - mean_q: 1.841 - mean_eps: 0.364 - ale.lives: 2.095\n",
      "\n",
      "Interval 122 (1210000 steps performed)\n",
      "10000/10000 [==============================] - 677s 68ms/step - reward: 0.0160\n",
      "12 episodes - episode_reward: 13.083 [5.000, 22.000] - loss: 0.015 - mae: 1.556 - mean_q: 1.866 - mean_eps: 0.359 - ale.lives: 2.012\n",
      "\n",
      "Interval 123 (1220000 steps performed)\n",
      "10000/10000 [==============================] - 697s 70ms/step - reward: 0.0191\n",
      "12 episodes - episode_reward: 15.750 [8.000, 24.000] - loss: 0.014 - mae: 1.559 - mean_q: 1.869 - mean_eps: 0.353 - ale.lives: 1.932\n",
      "\n",
      "Interval 124 (1230000 steps performed)\n",
      "10000/10000 [==============================] - 694s 69ms/step - reward: 0.0170\n",
      "14 episodes - episode_reward: 12.571 [6.000, 20.000] - loss: 0.014 - mae: 1.571 - mean_q: 1.884 - mean_eps: 0.348 - ale.lives: 2.102\n",
      "\n",
      "Interval 125 (1240000 steps performed)\n",
      "10000/10000 [==============================] - 695s 69ms/step - reward: 0.0169\n",
      "13 episodes - episode_reward: 12.846 [4.000, 18.000] - loss: 0.015 - mae: 1.583 - mean_q: 1.897 - mean_eps: 0.343 - ale.lives: 2.068\n",
      "\n",
      "Interval 126 (1250000 steps performed)\n",
      "10000/10000 [==============================] - 697s 70ms/step - reward: 0.0149\n",
      "14 episodes - episode_reward: 10.357 [3.000, 22.000] - loss: 0.016 - mae: 1.593 - mean_q: 1.910 - mean_eps: 0.338 - ale.lives: 2.107\n",
      "\n",
      "Interval 127 (1260000 steps performed)\n",
      "10000/10000 [==============================] - 697s 70ms/step - reward: 0.0179\n",
      "12 episodes - episode_reward: 14.250 [6.000, 21.000] - loss: 0.015 - mae: 1.616 - mean_q: 1.937 - mean_eps: 0.332 - ale.lives: 2.107\n",
      "\n",
      "Interval 128 (1270000 steps performed)\n",
      "10000/10000 [==============================] - 694s 69ms/step - reward: 0.0147\n",
      "12 episodes - episode_reward: 12.750 [6.000, 20.000] - loss: 0.017 - mae: 1.636 - mean_q: 1.960 - mean_eps: 0.327 - ale.lives: 1.960\n",
      "\n",
      "Interval 129 (1280000 steps performed)\n",
      "10000/10000 [==============================] - 700s 70ms/step - reward: 0.0161\n",
      "13 episodes - episode_reward: 12.923 [6.000, 22.000] - loss: 0.017 - mae: 1.653 - mean_q: 1.980 - mean_eps: 0.322 - ale.lives: 2.142\n",
      "\n",
      "Interval 130 (1290000 steps performed)\n",
      "10000/10000 [==============================] - 692s 69ms/step - reward: 0.0186\n",
      "9 episodes - episode_reward: 20.000 [14.000, 28.000] - loss: 0.017 - mae: 1.663 - mean_q: 1.994 - mean_eps: 0.317 - ale.lives: 1.954\n",
      "\n",
      "Interval 131 (1300000 steps performed)\n",
      "10000/10000 [==============================] - 696s 70ms/step - reward: 0.0198\n",
      "11 episodes - episode_reward: 16.909 [8.000, 32.000] - loss: 0.016 - mae: 1.660 - mean_q: 1.990 - mean_eps: 0.311 - ale.lives: 2.070\n",
      "\n",
      "Interval 132 (1310000 steps performed)\n",
      "10000/10000 [==============================] - 578s 58ms/step - reward: 0.0185\n",
      "12 episodes - episode_reward: 15.750 [7.000, 25.000] - loss: 0.018 - mae: 1.677 - mean_q: 2.010 - mean_eps: 0.306 - ale.lives: 2.003\n",
      "\n",
      "Interval 133 (1320000 steps performed)\n",
      "10000/10000 [==============================] - 552s 55ms/step - reward: 0.0170\n",
      "13 episodes - episode_reward: 14.154 [5.000, 26.000] - loss: 0.017 - mae: 1.679 - mean_q: 2.013 - mean_eps: 0.301 - ale.lives: 2.171\n",
      "\n",
      "Interval 134 (1330000 steps performed)\n",
      "10000/10000 [==============================] - 634s 63ms/step - reward: 0.0175\n",
      "12 episodes - episode_reward: 14.667 [6.000, 31.000] - loss: 0.017 - mae: 1.687 - mean_q: 2.022 - mean_eps: 0.295 - ale.lives: 1.984\n",
      "\n",
      "Interval 135 (1340000 steps performed)\n",
      "10000/10000 [==============================] - 581s 58ms/step - reward: 0.0163\n",
      "11 episodes - episode_reward: 14.818 [6.000, 27.000] - loss: 0.017 - mae: 1.705 - mean_q: 2.043 - mean_eps: 0.290 - ale.lives: 2.192\n",
      "\n",
      "Interval 136 (1350000 steps performed)\n",
      "10000/10000 [==============================] - 581s 58ms/step - reward: 0.0166\n",
      "11 episodes - episode_reward: 14.818 [6.000, 25.000] - loss: 0.017 - mae: 1.714 - mean_q: 2.054 - mean_eps: 0.285 - ale.lives: 1.935\n",
      "\n",
      "Interval 137 (1360000 steps performed)\n",
      "10000/10000 [==============================] - 580s 58ms/step - reward: 0.0186\n",
      "12 episodes - episode_reward: 15.417 [8.000, 26.000] - loss: 0.018 - mae: 1.713 - mean_q: 2.052 - mean_eps: 0.280 - ale.lives: 2.074\n",
      "\n",
      "Interval 138 (1370000 steps performed)\n",
      "10000/10000 [==============================] - 582s 58ms/step - reward: 0.0153\n",
      "10 episodes - episode_reward: 15.700 [8.000, 21.000] - loss: 0.018 - mae: 1.710 - mean_q: 2.051 - mean_eps: 0.274 - ale.lives: 2.103\n",
      "\n",
      "Interval 139 (1380000 steps performed)\n",
      "10000/10000 [==============================] - 584s 58ms/step - reward: 0.0152\n",
      "12 episodes - episode_reward: 12.583 [5.000, 24.000] - loss: 0.017 - mae: 1.708 - mean_q: 2.048 - mean_eps: 0.269 - ale.lives: 2.004\n",
      "\n",
      "Interval 140 (1390000 steps performed)\n",
      "10000/10000 [==============================] - 582s 58ms/step - reward: 0.0155\n",
      "11 episodes - episode_reward: 13.455 [7.000, 27.000] - loss: 0.016 - mae: 1.694 - mean_q: 2.030 - mean_eps: 0.264 - ale.lives: 1.978\n",
      "\n",
      "Interval 141 (1400000 steps performed)\n",
      "10000/10000 [==============================] - 578s 58ms/step - reward: 0.0163\n",
      "14 episodes - episode_reward: 12.143 [5.000, 28.000] - loss: 0.018 - mae: 1.717 - mean_q: 2.057 - mean_eps: 0.258 - ale.lives: 2.026\n",
      "\n",
      "Interval 142 (1410000 steps performed)\n",
      "10000/10000 [==============================] - 581s 58ms/step - reward: 0.0169\n",
      "12 episodes - episode_reward: 14.000 [5.000, 23.000] - loss: 0.018 - mae: 1.736 - mean_q: 2.081 - mean_eps: 0.253 - ale.lives: 2.126\n",
      "\n",
      "Interval 143 (1420000 steps performed)\n",
      "10000/10000 [==============================] - 579s 58ms/step - reward: 0.0185\n",
      "12 episodes - episode_reward: 15.417 [6.000, 34.000] - loss: 0.016 - mae: 1.731 - mean_q: 2.075 - mean_eps: 0.248 - ale.lives: 2.105\n",
      "\n",
      "Interval 144 (1430000 steps performed)\n",
      "10000/10000 [==============================] - 580s 58ms/step - reward: 0.0149\n",
      "12 episodes - episode_reward: 11.667 [2.000, 23.000] - loss: 0.018 - mae: 1.727 - mean_q: 2.069 - mean_eps: 0.243 - ale.lives: 2.045\n",
      "\n",
      "Interval 145 (1440000 steps performed)\n",
      "10000/10000 [==============================] - 582s 58ms/step - reward: 0.0168\n",
      "12 episodes - episode_reward: 14.083 [6.000, 22.000] - loss: 0.018 - mae: 1.735 - mean_q: 2.078 - mean_eps: 0.237 - ale.lives: 1.928\n",
      "\n",
      "Interval 146 (1450000 steps performed)\n",
      "10000/10000 [==============================] - 580s 58ms/step - reward: 0.0188\n",
      "11 episodes - episode_reward: 18.091 [10.000, 34.000] - loss: 0.018 - mae: 1.737 - mean_q: 2.082 - mean_eps: 0.232 - ale.lives: 2.066\n",
      "\n",
      "Interval 147 (1460000 steps performed)\n",
      "10000/10000 [==============================] - 579s 58ms/step - reward: 0.0169\n",
      "12 episodes - episode_reward: 13.583 [6.000, 26.000] - loss: 0.018 - mae: 1.742 - mean_q: 2.087 - mean_eps: 0.227 - ale.lives: 2.147\n",
      "\n",
      "Interval 148 (1470000 steps performed)\n",
      "10000/10000 [==============================] - 582s 58ms/step - reward: 0.0123\n",
      "15 episodes - episode_reward: 8.467 [6.000, 15.000] - loss: 0.018 - mae: 1.749 - mean_q: 2.096 - mean_eps: 0.222 - ale.lives: 2.055\n",
      "\n",
      "Interval 149 (1480000 steps performed)\n",
      "10000/10000 [==============================] - 579s 58ms/step - reward: 0.0158\n",
      "14 episodes - episode_reward: 10.786 [4.000, 18.000] - loss: 0.017 - mae: 1.759 - mean_q: 2.108 - mean_eps: 0.216 - ale.lives: 2.064\n",
      "\n",
      "Interval 150 (1490000 steps performed)\n",
      "10000/10000 [==============================] - 579s 58ms/step - reward: 0.0168\n",
      "11 episodes - episode_reward: 14.909 [5.000, 28.000] - loss: 0.018 - mae: 1.765 - mean_q: 2.115 - mean_eps: 0.211 - ale.lives: 1.915\n",
      "\n",
      "Interval 151 (1500000 steps performed)\n",
      "10000/10000 [==============================] - 581s 58ms/step - reward: 0.0179\n",
      "12 episodes - episode_reward: 14.417 [5.000, 21.000] - loss: 0.017 - mae: 1.762 - mean_q: 2.112 - mean_eps: 0.206 - ale.lives: 2.139\n",
      "\n",
      "Interval 152 (1510000 steps performed)\n",
      "10000/10000 [==============================] - 582s 58ms/step - reward: 0.0174\n",
      "14 episodes - episode_reward: 13.929 [6.000, 31.000] - loss: 0.018 - mae: 1.764 - mean_q: 2.116 - mean_eps: 0.200 - ale.lives: 2.128\n",
      "\n",
      "Interval 153 (1520000 steps performed)\n",
      "10000/10000 [==============================] - 579s 58ms/step - reward: 0.0178\n",
      "12 episodes - episode_reward: 14.000 [5.000, 22.000] - loss: 0.017 - mae: 1.763 - mean_q: 2.113 - mean_eps: 0.195 - ale.lives: 2.226\n",
      "\n",
      "Interval 154 (1530000 steps performed)\n",
      "10000/10000 [==============================] - 580s 58ms/step - reward: 0.0154\n",
      "12 episodes - episode_reward: 13.083 [4.000, 18.000] - loss: 0.018 - mae: 1.763 - mean_q: 2.114 - mean_eps: 0.190 - ale.lives: 2.153\n",
      "\n",
      "Interval 155 (1540000 steps performed)\n",
      "10000/10000 [==============================] - 579s 58ms/step - reward: 0.0159\n",
      "12 episodes - episode_reward: 13.917 [6.000, 19.000] - loss: 0.017 - mae: 1.771 - mean_q: 2.123 - mean_eps: 0.185 - ale.lives: 1.984\n",
      "\n",
      "Interval 156 (1550000 steps performed)\n",
      "10000/10000 [==============================] - 581s 58ms/step - reward: 0.0170\n",
      "11 episodes - episode_reward: 15.182 [2.000, 28.000] - loss: 0.018 - mae: 1.787 - mean_q: 2.142 - mean_eps: 0.179 - ale.lives: 2.093\n",
      "\n",
      "Interval 157 (1560000 steps performed)\n",
      "10000/10000 [==============================] - 579s 58ms/step - reward: 0.0174\n",
      "10 episodes - episode_reward: 15.900 [7.000, 28.000] - loss: 0.019 - mae: 1.776 - mean_q: 2.128 - mean_eps: 0.174 - ale.lives: 1.987\n",
      "\n",
      "Interval 158 (1570000 steps performed)\n",
      "10000/10000 [==============================] - 579s 58ms/step - reward: 0.0137\n",
      "12 episodes - episode_reward: 12.833 [6.000, 24.000] - loss: 0.018 - mae: 1.781 - mean_q: 2.133 - mean_eps: 0.169 - ale.lives: 1.949\n",
      "\n",
      "Interval 159 (1580000 steps performed)\n",
      "10000/10000 [==============================] - 578s 58ms/step - reward: 0.0162\n",
      "11 episodes - episode_reward: 13.455 [6.000, 25.000] - loss: 0.019 - mae: 1.781 - mean_q: 2.134 - mean_eps: 0.163 - ale.lives: 2.076\n",
      "\n",
      "Interval 160 (1590000 steps performed)\n",
      "10000/10000 [==============================] - 580s 58ms/step - reward: 0.0173\n",
      "11 episodes - episode_reward: 15.636 [10.000, 22.000] - loss: 0.017 - mae: 1.775 - mean_q: 2.128 - mean_eps: 0.158 - ale.lives: 2.010\n",
      "\n",
      "Interval 161 (1600000 steps performed)\n",
      "10000/10000 [==============================] - 579s 58ms/step - reward: 0.0161\n",
      "12 episodes - episode_reward: 13.667 [5.000, 20.000] - loss: 0.016 - mae: 1.779 - mean_q: 2.132 - mean_eps: 0.153 - ale.lives: 2.035\n",
      "\n",
      "Interval 162 (1610000 steps performed)\n",
      "10000/10000 [==============================] - 580s 58ms/step - reward: 0.0166\n",
      "10 episodes - episode_reward: 17.300 [9.000, 25.000] - loss: 0.016 - mae: 1.784 - mean_q: 2.141 - mean_eps: 0.148 - ale.lives: 2.069\n",
      "\n",
      "Interval 163 (1620000 steps performed)\n",
      "10000/10000 [==============================] - 579s 58ms/step - reward: 0.0161\n",
      "12 episodes - episode_reward: 12.750 [6.000, 27.000] - loss: 0.016 - mae: 1.787 - mean_q: 2.143 - mean_eps: 0.142 - ale.lives: 2.080\n",
      "\n",
      "Interval 164 (1630000 steps performed)\n",
      "10000/10000 [==============================] - 591s 59ms/step - reward: 0.0148\n",
      "14 episodes - episode_reward: 11.357 [4.000, 27.000] - loss: 0.015 - mae: 1.779 - mean_q: 2.134 - mean_eps: 0.137 - ale.lives: 2.039\n",
      "\n",
      "Interval 165 (1640000 steps performed)\n",
      "10000/10000 [==============================] - 582s 58ms/step - reward: 0.0163\n",
      "12 episodes - episode_reward: 13.167 [6.000, 23.000] - loss: 0.016 - mae: 1.804 - mean_q: 2.162 - mean_eps: 0.132 - ale.lives: 1.837\n",
      "\n",
      "Interval 166 (1650000 steps performed)\n",
      "10000/10000 [==============================] - 582s 58ms/step - reward: 0.0154\n",
      "12 episodes - episode_reward: 12.750 [6.000, 24.000] - loss: 0.017 - mae: 1.804 - mean_q: 2.163 - mean_eps: 0.127 - ale.lives: 2.072\n",
      "\n",
      "Interval 167 (1660000 steps performed)\n",
      "10000/10000 [==============================] - 580s 58ms/step - reward: 0.0181\n",
      "11 episodes - episode_reward: 17.273 [6.000, 26.000] - loss: 0.016 - mae: 1.800 - mean_q: 2.158 - mean_eps: 0.121 - ale.lives: 2.066\n",
      "\n",
      "Interval 168 (1670000 steps performed)\n",
      "10000/10000 [==============================] - 579s 58ms/step - reward: 0.0145\n",
      "13 episodes - episode_reward: 10.769 [4.000, 17.000] - loss: 0.016 - mae: 1.796 - mean_q: 2.153 - mean_eps: 0.116 - ale.lives: 1.957\n",
      "\n",
      "Interval 169 (1680000 steps performed)\n",
      "10000/10000 [==============================] - 582s 58ms/step - reward: 0.0171\n",
      "11 episodes - episode_reward: 15.545 [4.000, 23.000] - loss: 0.016 - mae: 1.793 - mean_q: 2.151 - mean_eps: 0.111 - ale.lives: 2.115\n",
      "\n",
      "Interval 170 (1690000 steps performed)\n",
      "10000/10000 [==============================] - 579s 58ms/step - reward: 0.0166\n",
      "12 episodes - episode_reward: 14.250 [5.000, 22.000] - loss: 0.016 - mae: 1.793 - mean_q: 2.148 - mean_eps: 0.105 - ale.lives: 1.992\n",
      "\n",
      "Interval 171 (1700000 steps performed)\n",
      "10000/10000 [==============================] - 582s 58ms/step - reward: 0.0158\n",
      "11 episodes - episode_reward: 12.909 [6.000, 19.000] - loss: 0.015 - mae: 1.777 - mean_q: 2.132 - mean_eps: 0.100 - ale.lives: 2.079\n",
      "\n",
      "Interval 172 (1710000 steps performed)\n",
      "10000/10000 [==============================] - 579s 58ms/step - reward: 0.0148\n",
      "10 episodes - episode_reward: 15.600 [8.000, 29.000] - loss: 0.016 - mae: 1.773 - mean_q: 2.126 - mean_eps: 0.095 - ale.lives: 1.940\n",
      "\n",
      "Interval 173 (1720000 steps performed)\n",
      "10000/10000 [==============================] - 581s 58ms/step - reward: 0.0160\n",
      "12 episodes - episode_reward: 13.667 [6.000, 23.000] - loss: 0.015 - mae: 1.789 - mean_q: 2.145 - mean_eps: 0.090 - ale.lives: 2.011\n",
      "\n",
      "Interval 174 (1730000 steps performed)\n",
      "10000/10000 [==============================] - 578s 58ms/step - reward: 0.0168\n",
      "12 episodes - episode_reward: 14.333 [5.000, 29.000] - loss: 0.015 - mae: 1.788 - mean_q: 2.144 - mean_eps: 0.084 - ale.lives: 2.131\n",
      "\n",
      "Interval 175 (1740000 steps performed)\n",
      "10000/10000 [==============================] - 580s 58ms/step - reward: 0.0117\n",
      "13 episodes - episode_reward: 8.308 [3.000, 13.000] - loss: 0.015 - mae: 1.788 - mean_q: 2.145 - mean_eps: 0.079 - ale.lives: 2.030\n",
      "\n",
      "Interval 176 (1750000 steps performed)\n",
      "10000/10000 [==============================] - 579s 58ms/step - reward: 0.0162\n",
      "11 episodes - episode_reward: 14.818 [5.000, 23.000] - loss: 0.016 - mae: 1.792 - mean_q: 2.148 - mean_eps: 0.074 - ale.lives: 1.886\n",
      "\n",
      "Interval 177 (1760000 steps performed)\n",
      "10000/10000 [==============================] - 580s 58ms/step - reward: 0.0179\n",
      "14 episodes - episode_reward: 13.286 [6.000, 22.000] - loss: 0.016 - mae: 1.803 - mean_q: 2.163 - mean_eps: 0.068 - ale.lives: 2.050\n",
      "\n",
      "Interval 178 (1770000 steps performed)\n",
      "10000/10000 [==============================] - 579s 58ms/step - reward: 0.0140\n",
      "13 episodes - episode_reward: 10.077 [6.000, 18.000] - loss: 0.014 - mae: 1.808 - mean_q: 2.169 - mean_eps: 0.063 - ale.lives: 2.201\n",
      "\n",
      "Interval 179 (1780000 steps performed)\n",
      "10000/10000 [==============================] - 580s 58ms/step - reward: 0.0185\n",
      "12 episodes - episode_reward: 14.583 [8.000, 23.000] - loss: 0.016 - mae: 1.816 - mean_q: 2.179 - mean_eps: 0.058 - ale.lives: 1.992\n",
      "\n",
      "Interval 180 (1790000 steps performed)\n",
      "10000/10000 [==============================] - 578s 58ms/step - reward: 0.0158\n",
      "11 episodes - episode_reward: 16.182 [3.000, 27.000] - loss: 0.015 - mae: 1.835 - mean_q: 2.201 - mean_eps: 0.053 - ale.lives: 2.014\n",
      "\n",
      "Interval 181 (1800000 steps performed)\n",
      "10000/10000 [==============================] - 579s 58ms/step - reward: 0.0151\n",
      "10 episodes - episode_reward: 14.300 [6.000, 24.000] - loss: 0.015 - mae: 1.833 - mean_q: 2.199 - mean_eps: 0.050 - ale.lives: 2.057\n",
      "\n",
      "Interval 182 (1810000 steps performed)\n",
      "10000/10000 [==============================] - 580s 58ms/step - reward: 0.0177\n",
      "12 episodes - episode_reward: 13.833 [7.000, 21.000] - loss: 0.015 - mae: 1.831 - mean_q: 2.198 - mean_eps: 0.050 - ale.lives: 2.080\n",
      "\n",
      "Interval 183 (1820000 steps performed)\n",
      "10000/10000 [==============================] - 580s 58ms/step - reward: 0.0172\n",
      "10 episodes - episode_reward: 18.700 [10.000, 30.000] - loss: 0.015 - mae: 1.832 - mean_q: 2.198 - mean_eps: 0.050 - ale.lives: 1.995\n",
      "\n",
      "Interval 184 (1830000 steps performed)\n",
      "10000/10000 [==============================] - 593s 59ms/step - reward: 0.0117\n",
      "11 episodes - episode_reward: 9.727 [5.000, 17.000] - loss: 0.015 - mae: 1.839 - mean_q: 2.205 - mean_eps: 0.050 - ale.lives: 1.829\n",
      "\n",
      "Interval 185 (1840000 steps performed)\n",
      "10000/10000 [==============================] - 571s 57ms/step - reward: 0.0155\n",
      "12 episodes - episode_reward: 13.250 [5.000, 22.000] - loss: 0.014 - mae: 1.855 - mean_q: 2.225 - mean_eps: 0.050 - ale.lives: 2.049\n",
      "\n",
      "Interval 186 (1850000 steps performed)\n",
      "10000/10000 [==============================] - 886s 89ms/step - reward: 0.0158\n",
      "12 episodes - episode_reward: 13.667 [5.000, 26.000] - loss: 0.015 - mae: 1.869 - mean_q: 2.242 - mean_eps: 0.050 - ale.lives: 2.009\n",
      "\n",
      "Interval 187 (1860000 steps performed)\n",
      "10000/10000 [==============================] - 1556s 155ms/step - reward: 0.0173\n",
      "12 episodes - episode_reward: 13.583 [5.000, 29.000] - loss: 0.015 - mae: 1.871 - mean_q: 2.246 - mean_eps: 0.050 - ale.lives: 2.160\n",
      "\n",
      "Interval 188 (1870000 steps performed)\n",
      "10000/10000 [==============================] - 759s 76ms/step - reward: 0.0169\n",
      "13 episodes - episode_reward: 12.154 [7.000, 16.000] - loss: 0.015 - mae: 1.883 - mean_q: 2.259 - mean_eps: 0.050 - ale.lives: 2.195\n",
      "\n",
      "Interval 189 (1880000 steps performed)\n",
      "10000/10000 [==============================] - 459s 46ms/step - reward: 0.0167\n",
      "12 episodes - episode_reward: 15.583 [5.000, 25.000] - loss: 0.014 - mae: 1.883 - mean_q: 2.258 - mean_eps: 0.050 - ale.lives: 2.172\n",
      "\n",
      "Interval 190 (1890000 steps performed)\n",
      "10000/10000 [==============================] - 457s 46ms/step - reward: 0.0160\n",
      "10 episodes - episode_reward: 15.700 [8.000, 21.000] - loss: 0.015 - mae: 1.891 - mean_q: 2.268 - mean_eps: 0.050 - ale.lives: 1.883\n",
      "\n",
      "Interval 191 (1900000 steps performed)\n",
      "10000/10000 [==============================] - 455s 45ms/step - reward: 0.0142\n",
      "12 episodes - episode_reward: 12.500 [4.000, 24.000] - loss: 0.017 - mae: 1.903 - mean_q: 2.281 - mean_eps: 0.050 - ale.lives: 1.929\n",
      "\n",
      "Interval 192 (1910000 steps performed)\n",
      "10000/10000 [==============================] - 451s 45ms/step - reward: 0.0138\n",
      "12 episodes - episode_reward: 11.250 [4.000, 24.000] - loss: 0.015 - mae: 1.904 - mean_q: 2.284 - mean_eps: 0.050 - ale.lives: 2.049\n",
      "\n",
      "Interval 193 (1920000 steps performed)\n",
      "10000/10000 [==============================] - 459s 46ms/step - reward: 0.0167\n",
      "12 episodes - episode_reward: 14.083 [6.000, 30.000] - loss: 0.016 - mae: 1.922 - mean_q: 2.305 - mean_eps: 0.050 - ale.lives: 2.103\n",
      "\n",
      "Interval 194 (1930000 steps performed)\n",
      "10000/10000 [==============================] - 455s 46ms/step - reward: 0.0170\n",
      "11 episodes - episode_reward: 13.909 [7.000, 26.000] - loss: 0.016 - mae: 1.917 - mean_q: 2.300 - mean_eps: 0.050 - ale.lives: 1.976\n",
      "\n",
      "Interval 195 (1940000 steps performed)\n",
      "10000/10000 [==============================] - 437s 44ms/step - reward: 0.0153\n",
      "12 episodes - episode_reward: 13.167 [4.000, 21.000] - loss: 0.019 - mae: 1.905 - mean_q: 2.285 - mean_eps: 0.050 - ale.lives: 2.051\n",
      "\n",
      "Interval 196 (1950000 steps performed)\n",
      "10000/10000 [==============================] - 596s 60ms/step - reward: 0.0140\n",
      "12 episodes - episode_reward: 12.667 [5.000, 29.000] - loss: 0.015 - mae: 1.920 - mean_q: 2.302 - mean_eps: 0.050 - ale.lives: 2.033\n",
      "\n",
      "Interval 197 (1960000 steps performed)\n",
      "10000/10000 [==============================] - 446s 45ms/step - reward: 0.0166\n",
      "11 episodes - episode_reward: 14.818 [10.000, 18.000] - loss: 0.016 - mae: 1.926 - mean_q: 2.309 - mean_eps: 0.050 - ale.lives: 1.977\n",
      "\n",
      "Interval 198 (1970000 steps performed)\n",
      "10000/10000 [==============================] - 446s 45ms/step - reward: 0.0150\n",
      "13 episodes - episode_reward: 11.846 [5.000, 22.000] - loss: 0.016 - mae: 1.927 - mean_q: 2.310 - mean_eps: 0.050 - ale.lives: 2.031\n",
      "\n",
      "Interval 199 (1980000 steps performed)\n",
      "10000/10000 [==============================] - 451s 45ms/step - reward: 0.0160\n",
      "11 episodes - episode_reward: 13.455 [11.000, 16.000] - loss: 0.015 - mae: 1.924 - mean_q: 2.308 - mean_eps: 0.050 - ale.lives: 2.040\n",
      "\n",
      "Interval 200 (1990000 steps performed)\n",
      "10000/10000 [==============================] - 445s 44ms/step - reward: 0.0153\n",
      "done, took 91631.164 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1760b279940>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.fit(env, \n",
    "        callbacks=callbacks, \n",
    "        nb_steps=2_000_000, \n",
    "        log_interval=10_000, \n",
    "        visualize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights(weights_filename, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación del modelo resultante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 19.000, steps: 926\n",
      "Episode 2: reward: 31.000, steps: 1284\n",
      "Episode 3: reward: 30.000, steps: 1165\n",
      "Episode 4: reward: 20.000, steps: 976\n",
      "Episode 5: reward: 17.000, steps: 746\n",
      "Episode 6: reward: 29.000, steps: 1261\n",
      "Episode 7: reward: 29.000, steps: 1309\n",
      "Episode 8: reward: 22.000, steps: 1122\n",
      "Episode 9: reward: 23.000, steps: 939\n",
      "Episode 10: reward: 18.000, steps: 858\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1fc9f2c1340>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing part to calculate the mean reward\n",
    "weights_filename = 'dqn_SpaceInvaders-v4_weights_2000000.h5f'   # .format(env_name)\n",
    "dqn.load_weights(weights_filename)\n",
    "dqn.test(env, nb_episodes=10, visualize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Justificación de los parámetros seleccionados y de los resultados obtenidos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta implementación de DQN se utilizan la biblioteca de keras-rl y gym en vez de gymnasium y pytorch.\n",
    "\n",
    "## Parametros de observación y procesado del entorno\n",
    "\n",
    "- Las dimensiones de entrada son de 84x84 y la ventana es de 4 frames, lo que indica que se toman 4 fotogramas consecutivos como entrada de la red \n",
    "- Clase atariprocessor: convierte las imagenes del juego en un formato adecuado para la red neuronal, lo que incluye redimensionar la imagen a 84x84, convertirla a escala de grises, se normaliza el lote de imágene,s y se limita o clipea la recompensa a un rango de -1 a 1.\n",
    "- Se define el entorno  'SpaceInvaders-v4' de OpenAI Gym.\n",
    "- El modelo consta de 6 acciones.\n",
    "\n",
    "## Parámetros del modelo \n",
    "\n",
    "Se construye un modelo secuencial con varias capas convolucionales y de pooling, seguidas de capas densas.\n",
    "\n",
    "- Capas Convolucionales y de Pooling: reducimos la dimensionalidad mientras aumentamos el numero de filtros.\n",
    "- Funcion de activación ReLU\n",
    "- La capa de salida tiene tantas neuronas como acciones disponibles y utiliza una activación lineal.\n",
    "\n",
    "## Parámetros de la policy y memoria\n",
    "\n",
    "Se utiliza una política epsilon greedy mediante la técnica simulated annealing: \n",
    "\n",
    "- empieza con un epsilón de 1.0, es decir el proceso de entrenamiento es completamente aleatorio ( fase de exploración ). \n",
    "- Disminuye linealmente hasta 0.05 en 1_800_000 mil steps.\n",
    "- En la fase de test también se utiliza un epsilón de 0.05 para mantener un margen de aleatoriedad o \"magia\".\n",
    "- Se establece un límite o buffer de memoria de 1,000,000, lo que significa que el agente puede almacenar hasta un millón de experiencias pasadas.\n",
    "\n",
    "## Configuración del agente\n",
    "\n",
    "- nb_steps_warmup: 20,000 pasos de \"calentamiento\" antes de que comience el entrenamiento activo, es decir, no se actualiza el modelo en este periodo.\n",
    "- Gamma (γ): Un discount factor de 0.99.\n",
    "- target_model_update: 10,000 pasos para la actualización del modelo objetivo.\n",
    "- train_interval: El agente realiza una acción de entrenamiento cada 4 pasos.\n",
    "\n",
    "## Entrenamiento\n",
    "\n",
    "- optimizador adam con un learning rate de 0.00025 \n",
    "- callbacks para guardar modelos intermendios cada 50 mil pasos\n",
    "- 2_000_000 de pasos para el entrenamiento en total. El entrenamiento a durado más de 25 horas.\n",
    "- La fase de explotación con un epsilón de 0.05 dura \n",
    "\n",
    "## Resultados\n",
    "\n",
    "La fase de testo muestra cierta inconsistencia (resultados dispersos), sin embargo, en la mayoría de casos llega sobradamente a los 20 puntos de media, cosa que no ocurria en los ensayos previos donde se ha entrenado el modelo con medio millón y después con un millón de steps.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
