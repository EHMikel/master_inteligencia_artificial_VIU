{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LSVPAihG4U1j"
   },
   "source": [
    "# Practica 0 - Introduccion a Gym\n",
    "\n",
    "> En la presente sesión, se va a realizar una introduccion a la libreria gym. Esta libreria de acceso abierto, desarrollada por OpenAI, ofrece entornos para distintos juegos de Atari. En dichos entornos, se pueden desarrollar, implementar, y comparar distintas soluciones de aprendizaje por refuerzo, de forma flexible.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWWcufoC7S2B"
   },
   "source": [
    "---\n",
    "## **PARTE 1** - Instalación y requisitos previos\n",
    "\n",
    "> Las prácticas han sido preparadas para poder realizarse en el entorno de trabajo de Google Colab. Sin embargo, esta plataforma presenta ciertas incompatibilidades a la hora de visualizar la renderización en gym. Por ello, para obtener estas visualizaciones, se deberá trasladar el entorno de trabajo a local. Por ello, el presente dosier presenta instrucciones para poder trabajar en ambos entornos. Siga los siguientes pasos para un correcto funcionamiento:\n",
    "1.   **LOCAL:** Preparar el enviroment, siguiendo las intrucciones detalladas en la sección *1.1.Preparar enviroment*.\n",
    "2.  **AMBOS:** Modificar las variables \"mount\" y \"drive_mount\" a la carpeta de trabajo en drive en el caso de estar en Colab, y ejecturar la celda *1.2.Localizar entorno de trabajo*.\n",
    "3. **COLAB:** se deberá ejecutar las celdas correspondientes al montaje de la carpeta de trabajo en Drive. Esta corresponde a la sección *1.3.Montar carpeta de datos local*.\n",
    "4.  **AMBOS:** Instalar las librerías necesarias, siguiendo la sección *1.4.Instalar librerías necesarias*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1svUw2WiJAUy"
   },
   "source": [
    "---\n",
    "### 1.1. Preparar enviroment (solo local)\n",
    "\n",
    "\n",
    "\n",
    "> Para preparar el entorno de trabajo en local, se han seguido los siguientes pasos:\n",
    "1. En Windows, puede ser necesario instalar las C++ Build Tools. Para ello, siga los siguientes pasos: https://towardsdatascience.com/how-to-install-openai-gym-in-a-windows-environment-338969e24d30.\n",
    "2. Instalar Anaconda\n",
    "3. Siguiendo el código que se presenta comentado en la próxima celda: Crear un enviroment, cambiar la ruta de trabajo, e instalar librerías básicas.\n",
    "\n",
    "\n",
    "```\n",
    "conda update --all\n",
    "conda create --name miar_rl python=3.8\n",
    "conda activate miar_rl\n",
    "cd \"PATH_TO_FOLDER\"\n",
    "conda install git\n",
    "pip install jupyter\n",
    "```\n",
    "\n",
    "\n",
    "4. Abrir la notebook con *jupyter-notebook*.\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "jupyter-notebook\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ouO30DIAKL3"
   },
   "source": [
    "---\n",
    "### 1.2. Localizar entorno de trabajo: Google colab o local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "cw5W3OopAFKN"
   },
   "outputs": [],
   "source": [
    "# ATENCIÓN!! Modificar ruta relativa a la práctica si es distinta (drive_root)\n",
    "mount='/content/gdrive'\n",
    "drive_root = mount + \"/My Drive/08_MIAR/sesiones_practicas/sesion_practica_0\"\n",
    "\n",
    "try:\n",
    "  from google.colab import drive\n",
    "  IN_COLAB=True\n",
    "except:\n",
    "  IN_COLAB=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sK5sY_ybAFt8"
   },
   "source": [
    "---\n",
    "### 1.3. Montar carpeta de datos local (solo Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3lN7KLe05NSa",
    "outputId": "670a65b1-8911-49ef-db62-2052a36cf545"
   },
   "outputs": [],
   "source": [
    "# # Switch to the directory on the Google Drive that you want to use\n",
    "# import os\n",
    "# if IN_COLAB:\n",
    "#   print(\"We're running Colab\")\n",
    "\n",
    "#   if IN_COLAB:\n",
    "#     # Mount the Google Drive at mount\n",
    "#     print(\"Colab: mounting Google drive on \", mount)\n",
    "\n",
    "#     drive.mount(mount)\n",
    "\n",
    "#     # Create drive_root if it doesn't exist\n",
    "#     create_drive_root = True\n",
    "#     if create_drive_root:\n",
    "#       print(\"\\nColab: making sure \", drive_root, \" exists.\")\n",
    "#       os.makedirs(drive_root, exist_ok=True)\n",
    "\n",
    "#     # Change to the directory\n",
    "#     print(\"\\nColab: Changing directory to \", drive_root)\n",
    "#     %cd $drive_root\n",
    "# # Verify we're in the correct working directory\n",
    "# %pwd\n",
    "# print(\"Archivos en el directorio: \")\n",
    "# print(os.listdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m2zVSAPW43MH"
   },
   "source": [
    "---\n",
    "### 1.4. Instalar librerías necesarias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AmVd8L9XBt26",
    "outputId": "156fe7f5-74d8-46d9-f02c-b1d729fce4bb"
   },
   "outputs": [],
   "source": [
    "# if IN_COLAB:\n",
    "#   %pip install gym==0.17.3\n",
    "#   %pip install git+https://github.com/Kojoley/atari-py.git\n",
    "#   %pip install keras-rl2==1.0.5\n",
    "#   %pip install tensorflow==2.8\n",
    "# else:\n",
    "#   %pip install --upgrade pip\n",
    "#   %pip install gym==0.17.3\n",
    "#   %pip install git+https://github.com/Kojoley/atari-py.git\n",
    "#   %pip install pyglet==1.5.0\n",
    "#   %pip install h5py==3.1.0\n",
    "#   %pip install Pillow==9.5.0\n",
    "#   %pip install keras-rl2==1.0.5\n",
    "#   %pip install Keras==2.2.4\n",
    "#   %pip install tensorflow==2.5.3\n",
    "#   %pip install torch==2.0.1\n",
    "#   %pip install agents==1.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TPYIGhZA43ML"
   },
   "source": [
    "---\n",
    "## **PARTE 2** - *Librerias de RL*\n",
    "\n",
    "> A continuacion, se introducen una serie de librerias para desarrollar soluciones de deep learning para el aprendizaje profundo.\n",
    "\n",
    ">**Librerias focalizadas en implementar entrenamiento:**\n",
    "*   Tensorflow: https://www.tensorflow.org/\n",
    "*   Keras-rl: https://github.com/keras-rl/keras-rl\n",
    "*   Pytorch: https://pytorch.org/\n",
    "*   SpinningUp: https://spinningup.openai.com/en/latest/\n",
    "\n",
    ">**Librerias centradas en trabajar con entornos:**\n",
    "*   Gym: https://gym.openai.com/\n",
    "*   Retro: https://github.com/openai/retro\n",
    "*   Unity: https://unity.com/es/products/machine-learning-agents\n",
    "*   Carla:  https://github.com/carla-simulator/carla\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pgmnaMBw7_0l"
   },
   "source": [
    "\n",
    "## **PARTE 3** - *Ejemplo de entornos con gym*\n",
    "\n",
    "### AGENTE ALEATORIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "5ws7qRdme6g4"
   },
   "outputs": [],
   "source": [
    "import gym  # Importamos libreria - Si no os salta error, lo tenéis bien instalado. Especial Atención a las librerías de C++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "kIw5hKnye4Oj"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[123, 151010689]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creamos el entorno donde se va a ejecutar nuestro agente\n",
    "env = gym.make(\"BreakoutDeterministic-v4\")\n",
    "env.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones del espacion de observaciones\n",
      "(210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "# observamos dimennsiones de espacio de observaciones y acciones\n",
    "print(\"Dimensiones del espacion de observaciones\")\n",
    "print(env.observation_space.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimensiones del espacio de acciones\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Discrete(4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# observamos el espacio de acciones\n",
    "print(\"dimensiones del espacio de acciones\")\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucle básico de ejecución\n",
    "env = gym.make(\"BreakoutDeterministic-v4\")\n",
    "observation = env.reset()                    # resetear el entorno inicial\n",
    "steps= 3000                                    # iteraciones a realizar dentro de la trayectoria limite máximo\n",
    "\n",
    "for _ in range(steps):\n",
    "    # renderizar - visualizar steps\n",
    "    if not IN_COLAB:\n",
    "        env.render()\n",
    "\n",
    "    action = env.action_space.sample()       # seleccionamos accion aleatoria\n",
    "    observation, reward, done, info = env.step(action)   # pasarle al entorno la accion seleccionada por el agente\n",
    "\n",
    "    if done: \n",
    "        observation = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recompensa obtenida en el episodio: 1.0\n",
      "Recompensa obtenida en el episodio: 2.0\n",
      "Recompensa obtenida en el episodio: 1.0\n",
      "Recompensa obtenida en el episodio: 0.0\n",
      "Recompensa obtenida en el episodio: 0.0\n",
      "Recompensa obtenida en el episodio: 1.0\n",
      "Recompensa obtenida en el episodio: 0.0\n",
      "Recompensa obtenida en el episodio: 1.0\n",
      "Recompensa obtenida en el episodio: 0.0\n",
      "Recompensa obtenida en el episodio: 2.0\n",
      "Recompensa obtenida en el episodio: 0.0\n",
      "Recompensa obtenida en el episodio: 0.0\n",
      "Recompensa obtenida en el episodio: 2.0\n",
      "Recompensa obtenida en el episodio: 1.0\n",
      "Recompensa obtenida en el episodio: 0.0\n",
      "Recompensa obtenida en el episodio: 3.0\n",
      "Recompensa obtenida en el episodio: 0.0\n",
      "Recompensa obtenida en el episodio: 1.0\n",
      "Recompensa obtenida en el episodio: 0.0\n",
      "Recompensa obtenida en el episodio: 1.0\n",
      "Recompensa obtenida en el episodio: 0.0\n",
      "Recompensa obtenida en el episodio: 1.0\n",
      "Recompensa obtenida en el episodio: 1.0\n",
      "Recompensa obtenida en el episodio: 2.0\n",
      "Recompensa obtenida en el episodio: 2.0\n",
      "Recompensa obtenida en el episodio: 0.0\n",
      "Recompensa obtenida en el episodio: 0.0\n",
      "Recompensa obtenida en el episodio: 5.0\n",
      "Recompensa obtenida en el episodio: 1.0\n",
      "Recompensa obtenida en el episodio: 1.0\n",
      "Recompensa obtenida en el episodio: 2.0\n",
      "Recompensa obtenida en el episodio: 1.0\n",
      "Recompensa obtenida en el episodio: 0.0\n",
      "Recompensa obtenida en el episodio: 0.0\n",
      "Recompensa obtenida en el episodio: 0.0\n",
      "Recompensa obtenida en el episodio: 0.0\n",
      "Recompensa obtenida en el episodio: 0.0\n",
      "Recompensa obtenida en el episodio: 3.0\n",
      "Recompensa obtenida en el episodio: 0.0\n",
      "Recompensa obtenida en el episodio: 1.0\n",
      "Recompensa obtenida en el episodio: 2.0\n",
      "Recompensa obtenida en el episodio: 1.0\n",
      "Recompensa obtenida en el episodio: 0.0\n",
      "Recompensa obtenida en el episodio: 0.0\n",
      "Recompensa obtenida en el episodio: 3.0\n",
      "Recompensa obtenida en el episodio: 0.0\n",
      "Recompensa obtenida en el episodio: 1.0\n",
      "Recompensa obtenida en el episodio: 1.0\n",
      "Recompensa obtenida en el episodio: 1.0\n",
      "Recompensa obtenida en el episodio: 1.0\n",
      "Recompensa obtenida en el episodio: 2.0\n",
      "Recompensa obtenida en el episodio: 1.0\n",
      "Recompensa obtenida en el episodio: 3.0\n",
      "Recompensa obtenida en el episodio: 2.0\n",
      "Recompensa obtenida en el episodio: 3.0\n",
      "Recompensa obtenida en el episodio: 1.0\n",
      "Recompensa obtenida en el episodio: 2.0\n",
      "Recompensa obtenida en el episodio: 1.0\n",
      "Recompensa obtenida en el episodio: 0.0\n",
      "Recompensa obtenida en el episodio: 1.0\n",
      "Recompensa obtenida en el episodio: 2.0\n",
      "Recompensa obtenida en el episodio: 0.0\n",
      "Recompensa obtenida en el episodio: 4.0\n",
      "Recompensa obtenida en el episodio: 1.0\n",
      "Recompensa obtenida en el episodio: 1.0\n",
      "Recompensa obtenida en el episodio: 4.0\n",
      "Recompensa obtenida en el episodio: 0.0\n",
      "Recompensa obtenida en el episodio: 2.0\n",
      "Recompensa obtenida en el episodio: 0.0\n",
      "Recompensa obtenida en el episodio: 0.0\n",
      "Recompensa obtenida en el episodio: 0.0\n",
      "Recompensa obtenida en el episodio: 0.0\n",
      "Recompensa obtenida en el episodio: 3.0\n",
      "Recompensa obtenida en el episodio: 0.0\n",
      "Recompensa obtenida en el episodio: 0.0\n",
      "Recompensa obtenida en el episodio: 0.0\n",
      "Recompensa obtenida en el episodio: 2.0\n",
      "Recompensa obtenida en el episodio: 1.0\n",
      "Recompensa obtenida en el episodio: 0.0\n",
      "Recompensa obtenida en el episodio: 3.0\n",
      "Recompensa obtenida en el episodio: 0.0\n",
      "Recompensa obtenida en el episodio: 3.0\n",
      "Recompensa obtenida en el episodio: 0.0\n",
      "Recompensa obtenida en el episodio: 2.0\n",
      "Recompensa obtenida en el episodio: 2.0\n",
      "Recompensa obtenida en el episodio: 0.0\n",
      "Recompensa obtenida en el episodio: 1.0\n",
      "Recompensa obtenida en el episodio: 1.0\n",
      "Recompensa obtenida en el episodio: 1.0\n",
      "Recompensa obtenida en el episodio: 3.0\n",
      "Recompensa obtenida en el episodio: 2.0\n",
      "Recompensa obtenida en el episodio: 1.0\n",
      "Recompensa obtenida en el episodio: 2.0\n",
      "Recompensa obtenida en el episodio: 1.0\n",
      "Recompensa obtenida en el episodio: 2.0\n",
      "Recompensa obtenida en el episodio: 2.0\n",
      "Recompensa obtenida en el episodio: 2.0\n",
      "Recompensa obtenida en el episodio: 2.0\n",
      "Recompensa obtenida en el episodio: 0.0\n",
      "Recompensa obtenida en el episodio: 0.0\n"
     ]
    }
   ],
   "source": [
    "# bucle bueno para entrenar\n",
    "env = gym.make(\"BreakoutDeterministic-v4\")\n",
    "observation = env.reset()\n",
    "\n",
    "episodes = 100\n",
    "steps_max = 1000\n",
    "\n",
    "for episode in range(episodes): \n",
    "    done = False\n",
    "    episode_rewar = 0   # calculamos la recompensa del episodio\n",
    "\n",
    "    step_i = 0\n",
    "    while not done: \n",
    "        step_i += 1\n",
    "        # renderizar - visualizar steps\n",
    "        if not IN_COLAB:\n",
    "            env.render()\n",
    "\n",
    "        # seleccionamos accion aleatoriamente\n",
    "        action = env.action_space.sample()\n",
    "        # Pasarel al etorno la accion seleccionada por el agente\n",
    "        observation, reward, done, info = env.step(action)\n",
    "\n",
    "        episode_rewar += reward\n",
    "\n",
    "        if done: \n",
    "            observation = env.reset()\n",
    "        if steps_max < step_i: \n",
    "            done= True\n",
    "            observation = env.reset()\n",
    "\n",
    "    print(f'Recompensa obtenida en el episodio: {episode_rewar}')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
